{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np                \n",
    "import transformers   \n",
    "import torch  \n",
    "import warnings \n",
    "warnings.simplefilter('ignore')   \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "# # Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run \n",
    "# these are 56 unique functions currently in this dataset\n",
    "unique_functions = ['pre sales','banking','legal','iot','branding','controller','testing','engineering','training','admin','security','data engineering','cyber security','automation','operations','marketing','infrastructure','digital','learning','tax','production','digital marketing','manufacturing','hr','purchase','devops','product management','applications','product security','solutions','inside sales','research','hiring','accounts','risk','artificial intelligence','constomer service','support','compliance','media','accounts recievable','data','blockchain','payroll','sales','network security','accounts payable','analytics','cloud','fraud','corporate finance','distribution','social media','it','account management','finance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.max_len = max_len\n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        text = text\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=56, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertModel\n",
    "from torch.nn import Linear, Dropout, Tanh, Sigmoid\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "  \n",
    "# Creating the customized distillbert model\n",
    "\n",
    "class DistilBERTClass(torch.nn.Module,PyTorchModelHubMixin):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "         \n",
    "        # unFreeze all layers\n",
    "        for name, param in self.l1.named_parameters():\n",
    "                     param.requires_grad = True\n",
    "             \n",
    "         \n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(768, 56)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0] \n",
    "        pooled_output = hidden_state[:, 0]\n",
    "        pooled_output = self.pre_classifier(pooled_output)  \n",
    "        pooled_output = torch.nn.Tanh()(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        output = self.classifier(pooled_output) \n",
    "        output = torch.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "model = DistilBERTClass()\n",
    "#model = torch.nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = DistilBERTClass.from_pretrained('anushkaSingh/distillbert_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "def validation(testing_loader):\n",
    "    model1.eval()\n",
    "    fin_outputs=[]   \n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            text = data['text']\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model1(ids, mask, token_type_ids)\n",
    "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist()) \n",
    "    return  fin_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "#actually this function is for test data not validation (dont get confused with the function name)\n",
    "def validation(testing_loader):\n",
    "    model1.eval()\n",
    "    fin_outputs=[]   \n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            outputs = model1(ids, mask, token_type_ids)\n",
    "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist()) \n",
    "    return fin_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "def flat_accuracy(preds, labels):\n",
    "    res = np.zeros(labels.shape[0])\n",
    "    for i in range(labels.shape[0]):\n",
    "        res[i] = np.all(preds[i] == labels[i]) \n",
    "    return np.sum(res) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_functional_label(text):\n",
    "    data = [[text]]\n",
    "    test_data = pd.DataFrame(data, columns=['text'])\n",
    "    testing_set = MultiLabelDataset(test_data, tokenizer, 512)\n",
    "    test_params = {'batch_size': 1,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "    testing_loader = DataLoader(testing_set, **test_params) \n",
    "    outputs= validation(testing_loader) \n",
    "    final_outputs = np.array(outputs) >=0.5\n",
    "    text_outputs = dict()\n",
    "    for i,output in enumerate(final_outputs[0]):#run\n",
    "      if output == True:\n",
    "         text_outputs[unique_functions[i]] = outputs[0][i]\n",
    "    if len(text_outputs) == 0:\n",
    "       max_value = max(outputs)\n",
    "       max_index = outputs.index(max_value)\n",
    "       text_outputs[unique_functions[max_index]] = outputs[0][max_index]   \n",
    "    return text_outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test on a single text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'engineering': 0.9974194765090942,\n",
       " 'artificial intelligence': 0.8907726407051086}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    " clodura.ai\n",
    "'''\n",
    "\n",
    "find_functional_label(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
